{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "import codecs\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建停用词表\n",
    "def stopwords_list(filepath):\n",
    "    stop_words_file = filepath\n",
    "    stopwords = codecs.open(stop_words_file,'r',encoding='utf8').readlines()\n",
    "    stopwords_list = [words.strip() for words in stopwords]\n",
    "    stop_flag = ['x', 'c', 'u','d', 'p', 't', 'uj', 'm', 'f', 'r']\n",
    "    return stopwords_list,stop_flag\n",
    "\n",
    "#对一篇文章，或者句子分词、去停用词\n",
    "def tokenization(filename):\n",
    "    res = []\n",
    "    with open(filename,'r') as f:\n",
    "        text = f.read()\n",
    "        words = pseg.cut(text)\n",
    "    stopwordslist,stop_flag = stopwords_list('/Users/taoxudong/huli/test/stop_words.txt')\n",
    "    #分词特性？中文对应着stop_flag的含义。？？？？？？\n",
    "    for word,flag in words:\n",
    "        if word not in stopwordslist and flag not in stop_flag:\n",
    "            if word != '\\t':\n",
    "                res.append(word)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将初步分词结果写入文件\n",
    "def writeCorpus(fileName,contents):\n",
    "    fp = open(fileName,'w+')\n",
    "    fp.write(str(contents))\n",
    "    fp.close()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#遍历初步分词的结果，进行非重复词的hash编码，2*32位应该足够，将idf作为权重进行运算，分别存入两个字典中\n",
    "#设置所有词的hash值\n",
    "def setHashAndWeight(path):\n",
    "    f = codecs.open(path, 'r', encoding='utf-8')\n",
    "    dictHash = dict()\n",
    "    dictWeight = dict()\n",
    "    i = 0 #hash编码\n",
    "    lines = 0#记录文本数量,以计算idf值\n",
    "    #遍历文本，进行hash编码和统计df词频(在多少篇文章出现过，而不是总词频，\\\n",
    "    #比如某个词在一个文本中出现三次也只算一次)\n",
    "    for line in f:\n",
    "        lines +=1\n",
    "        text_noRepeate = set(str(line).strip().split())\n",
    "        for item in text_noRepeate:\n",
    "            if item not in dictWeight:\n",
    "                dictWeight[item] = 1\n",
    "                dictHash[item] = i\n",
    "                i += 1\n",
    "            else:\n",
    "                dictWeight[item] += 1\n",
    "    f.close()\n",
    "    del i\n",
    "    #hash编码转为array形式的二进制，方便计算\n",
    "    for item in dictHash:\n",
    "        L = list(bin(dictHash[item]))[2:]\n",
    "        intL = [int(x) for x in L]\n",
    "        for i in range(len(intL)):\n",
    "            if intL[i] == 0:\n",
    "                intL[i] = -1\n",
    "        intL = (BITS - len(intL))*[-1]+intL\n",
    "        dictHash[item] = np.array(intL)\n",
    "   #根据词频计算idf值\n",
    "    for item in dictWeight:\n",
    "        dictWeight[item] = math.log(lines/dictWeight[item])\n",
    "    \n",
    "    return dictHash,dictWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据词的hash对句子进行hash编码\n",
    "BITS = 31\n",
    "def senHash(sen,dictHash,dictWeight):\n",
    "    senHashCode = np.zeros(BITS)\n",
    "    temp = set(str(sen).strip().split())\n",
    "    #temp = str(sen).strip().split()\n",
    "    for item in temp:\n",
    "        senHashCode += dictHash[item]*dictWeight[item]\n",
    "    for i in range(BITS):\n",
    "        if senHashCode[i] > 0:\n",
    "            senHashCode[i] = 1\n",
    "        else:\n",
    "            senHashCode[i] = 0\n",
    "    return senHashCode\n",
    "\n",
    "#获取两个句子的Hamming distance，dis越小说明相似度越高\n",
    "def sen2senDis(sen1, sen2,dictHash,dictWeight):\n",
    "    temp1 = senHash(sen1,dictHash,dictWeight)\n",
    "    temp2 = senHash(sen2,dictHash,dictWeight)\n",
    "    Hamming = 0\n",
    "    for i in range(BITS):\n",
    "        if temp1[i] != temp2[i]:\n",
    "            Hamming += 1\n",
    "    return Hamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"'发生率']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-059b3183c090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#设置所有词的hash值和权重\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdictHash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictWeight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetHashAndWeight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./text_corpus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhaming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msen2senDis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictHash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictWeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-96-2ecb3c9b5a0b>\u001b[0m in \u001b[0;36msen2senDis\u001b[0;34m(sen1, sen2, dictHash, dictWeight)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#获取两个句子的Hamming distance，dis越小说明相似度越高\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msen2senDis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictHash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictWeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtemp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msenHash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictHash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictWeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtemp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msenHash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictHash\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdictWeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mHamming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-2ecb3c9b5a0b>\u001b[0m in \u001b[0;36msenHash\u001b[0;34m(sen, dictHash, dictWeight)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#temp = str(sen).strip().split()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0msenHashCode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdictHash\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdictWeight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBITS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msenHashCode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"'发生率']\""
     ]
    }
   ],
   "source": [
    "s1 = tokenization('/Users/taoxudong/huli/test/高血压.txt')\n",
    "s2 = tokenization('/Users/taoxudong/huli/test/低血压.txt')\n",
    "s1_content = writeCorpus('test1',s1)\n",
    "s2_content = writeCorpus('test2',s2)\n",
    "#设置所有词的hash值和权重\n",
    "dictHash,dictWeight = setHashAndWeight('./text_corpus')\n",
    "haming = sen2senDis(s1,s2,dictHash,dictWeight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
