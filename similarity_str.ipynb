{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Edit_distance_str(str1,str2):\n",
    "    edit_distance = Levenshtein.distance(str1,str2)\n",
    "    similarity = 1-(edit_distance/max(len(str1),len(str2)))\n",
    "    return {'distance':edit_distance,'similarity':similarity}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Edit_distance_str('搜易贷','搜易贷（北京）金融信息服务有限公司')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distance': 14, 'similarity': 0.17647058823529416}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Edit_distance_str('搜易贷','青岛搜易贷经济信息咨询有限公司')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'distance': 12, 'similarity': 0.19999999999999996}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n计算相似度，前提是：将文本转化为向量\\n使用jieba分词，词袋模型，tf-idf模型\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "计算相似度，前提是：将文本转化为向量\n",
    "使用jieba分词，词袋模型，tf-idf模型，LSI模型 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "import codecs\n",
    "from gensim import corpora,models,similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['搜易', '贷', '北京', '金融信息', '服务', '有限公司'], ['北京', '搜狐', '互联网', '信息', '服务', '有限公司'], ['青岛', '搜易', '贷', '经济', '信息', '咨询', '有限公司']]\n"
     ]
    }
   ],
   "source": [
    "#构建停用词表\n",
    "def stopwords_list(filepath):\n",
    "    stop_words_file = filepath\n",
    "    stopwords = codecs.open(stop_words_file,'r',encoding='utf8').readlines()\n",
    "    stopwords_list = [words.strip() for words in stopwords]\n",
    "    stop_flag = ['x', 'c', 'u','d', 'p', 't', 'uj', 'm', 'f', 'r']\n",
    "    return stopwords_list,stop_flag\n",
    "\n",
    "#对一篇文章，或者句子分词、去停用词\n",
    "def tokenization(filename):\n",
    "    res = [] \n",
    "    with open(filename,'r') as f:\n",
    "        text = f.read()\n",
    "        words = pseg.cut(text)\n",
    "    stopwordslist,stop_flag = stopwords_list('/Users/taoxudong/huli/test/stop_words.txt')\n",
    "    #分词特性？中文对应着stop_flag的含义。？？？？？？\n",
    "    for word,flag in words:\n",
    "        if word not in stopwordslist and flag not in stop_flag:\n",
    "            if word != '\\t':\n",
    "                res.append(word)\n",
    "    return res\n",
    "\n",
    "#加载要处理的文本，进行分词\n",
    "filenames = ['/Users/taoxudong/huli/test/测试数据二.txt',\n",
    "            '/Users/taoxudong/huli/test/测试数据三.txt',\n",
    "            '/Users/taoxudong/huli/test/测试数据四.txt']\n",
    "\n",
    "corpus = []\n",
    "for each in filenames:\n",
    "    corpus.append(tokenization(each))\n",
    "#建立词袋模型，生成字典和向量语料\n",
    "print(corpus)\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "doc_vectors = [dictionary.doc2bow(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['搜易', '贷']\n",
      "[(0, 0.41993365), (1, 0.0), (2, 0.28270504)]\n"
     ]
    }
   ],
   "source": [
    "#建立tf-idf模型\n",
    "tfidf = models.TfidfModel(doc_vectors)\n",
    "tfidf_vectors = tfidf[doc_vectors]\n",
    "#len(tfidf_vectors) = 文本数  len(tfidf_vectors[0] == 第一个文本中的词数量)\n",
    "print(len(tfidf_vectors))\n",
    "query = tokenization('/Users/taoxudong/huli/test/测试数据一.txt')\n",
    "print(query)\n",
    "query_bow = dictionary.doc2bow(query)\n",
    "index = similarities.MatrixSimilarity(tfidf_vectors)\n",
    "sims = index[query_bow]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['搜易', '贷']\n",
      "[(0, 0.84972703), (1, 0.29139632), (2, 0.89929235)]\n"
     ]
    }
   ],
   "source": [
    "#构建LSI模型，设置主题数（类数）\n",
    "lsi = models.LsiModel(tfidf_vectors,id2word = dictionary,num_topics=2)\n",
    "lsi_vector = lsi[tfidf_vectors]\n",
    "#LSI向量空间中，所有文本的向量都是二维的\n",
    "query = tokenization('/Users/taoxudong/huli/test/测试数据一.txt')\n",
    "query_bow = dictionary.doc2bow(query)\n",
    "query_lsi = lsi[query_bow]\n",
    "print(query)\n",
    "index = similarities.MatrixSimilarity(lsi_vector)\n",
    "sims = index[query_lsi]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
